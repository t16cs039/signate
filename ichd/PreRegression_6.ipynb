{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreRegression_6.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Y3jSE8nyVfc6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1570587957203,"user_tz":-540,"elapsed":17670,"user":{"displayName":"0 0","photoUrl":"","userId":"10300123485870236269"}},"outputId":"6e34955c-8f53-4d04-a1ea-c01f0fc6a50d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gZWaN8FVR7jQ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfvGGtqYSG4x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1570588042155,"user_tz":-540,"elapsed":11958,"user":{"displayName":"0 0","photoUrl":"","userId":"10300123485870236269"}},"outputId":"6f301364-5683-4391-a83e-d0a27d3e9af1"},"source":["# 学習用データの読み込み\n","df1 = pd.read_csv('train_genba.tsv', sep='\\t')\n","df2 = pd.read_csv('train_goto.tsv', sep='\\t')\n","df = pd.merge(df1, df2, on='pj_no', how='outer')\n","\n","del df1\n","del df2\n","\n","# 評価用データの読み込み\n","dftest1 = pd.read_csv('test_genba.tsv', sep='\\t')\n","dftest2 = pd.read_csv('test_goto.tsv', sep='\\t')\n","dftest = pd.merge(dftest1, dftest2, on='pj_no', how='outer')\n","\n","del dftest1\n","del dftest2\n","\n","# 提出用id\n","dftest_id = pd.DataFrame()\n","dftest_id = dftest['id'] \n","\n","# 契約金額(keiyaku_pr)\n","df_keiyaku_pr = pd.DataFrame()\n","df_keiyaku_pr = df['keiyaku_pr']\n","\n","df = df.drop('keiyaku_pr', axis=1)\n","\n","# 学習用データの個別要因(kobetsu1, kobetsu2, kobetsu3, kobetsu4)\n","df_kobetsu = pd.DataFrame()\n","\n","df_kobetsu = pd.concat(objs=[df_kobetsu, df['kobetsu1']], axis=1)\n","df_kobetsu = pd.concat(objs=[df_kobetsu, df['kobetsu2']], axis=1)\n","df_kobetsu = pd.concat(objs=[df_kobetsu, df['kobetsu3']], axis=1)\n","df_kobetsu = pd.concat(objs=[df_kobetsu, df['kobetsu4']], axis=1)\n","df = df.drop('kobetsu1', axis=1)\n","df = df.drop('kobetsu2', axis=1)\n","df = df.drop('kobetsu3', axis=1)\n","df = df.drop('kobetsu4', axis=1)\n","\n","df_kobetsu = df_kobetsu.fillna(value=0)\n","\n","# 評価用データの個別要因(kobetsu1, kobetsu2, kobetsu3, kobetsu4)\n","dftest_kobetsu = pd.DataFrame()\n","\n","dftest_kobetsu = pd.concat(objs=[dftest_kobetsu, dftest['kobetsu1']], axis=1)\n","dftest_kobetsu = pd.concat(objs=[dftest_kobetsu, dftest['kobetsu2']], axis=1)\n","dftest_kobetsu = pd.concat(objs=[dftest_kobetsu, dftest['kobetsu3']], axis=1)\n","dftest_kobetsu = pd.concat(objs=[dftest_kobetsu, dftest['kobetsu4']], axis=1)\n","dftest = dftest.drop('kobetsu1', axis=1)\n","dftest = dftest.drop('kobetsu2', axis=1)\n","dftest = dftest.drop('kobetsu3', axis=1)\n","dftest = dftest.drop('kobetsu4', axis=1)\n","\n","dftest_kobetsu = dftest_kobetsu.fillna(value=0)\n","\n","# 学習用個別データ(df_kobetsu)の前処理\n","df_kobetsu_result = pd.DataFrame()\n","\n","for elem in ['高圧線下','信号近い','信号前','横断歩道前','踏切付近','ごみ置き場前','心理的瑕疵あり','計画道路','地役権有','敷延2ｍ絞りあり','宅内高低差あり','嫌悪施設隣接','アパート南隣','街道沿い','交通量多い','裏道','行き止まり','行き止まり途中','車進入困難','前面道が坂途中','眺望良','床暖房付','エネファーム付','角地','二方路','三方路']:\n","    df_kobetsu_result[elem] = df_kobetsu.apply(lambda xs: 1 if elem in xs.values else 0, axis=1)\n","\n","# 評価用個別データ(dftest_kobetsu)の前処理\n","dftest_kobetsu_result = pd.DataFrame()\n","\n","for elem in ['高圧線下','信号近い','信号前','横断歩道前','踏切付近','ごみ置き場前','心理的瑕疵あり','計画道路','地役権有','敷延2ｍ絞りあり','宅内高低差あり','嫌悪施設隣接','アパート南隣','街道沿い','交通量多い','裏道','行き止まり','行き止まり途中','車進入困難','前面道が坂途中','眺望良','床暖房付','エネファーム付','角地','二方路','三方路']:\n","    dftest_kobetsu_result[elem] = dftest_kobetsu.apply(lambda xs: 1 if elem in xs.values else 0, axis=1)\n","\n","df = pd.concat(objs=[df, df_kobetsu_result], axis=1)\n","dftest = pd.concat(objs=[dftest, dftest_kobetsu_result], axis=1)\n","\n","del df_kobetsu\n","del dftest_kobetsu\n","del df_kobetsu_result\n","del dftest_kobetsu_result\n","\n","# 学習用データのその他規制(hokakisei1, hokakisei2, hokakisei3, hokakisei4)\n","df_hokakisei = pd.DataFrame()\n","\n","df_hokakisei = pd.concat(objs=[df_hokakisei, df['hokakisei1']], axis=1)\n","df_hokakisei = pd.concat(objs=[df_hokakisei, df['hokakisei2']], axis=1)\n","df_hokakisei = pd.concat(objs=[df_hokakisei, df['hokakisei3']], axis=1)\n","df_hokakisei = pd.concat(objs=[df_hokakisei, df['hokakisei4']], axis=1)\n","df = df.drop('hokakisei1', axis=1)\n","df = df.drop('hokakisei2', axis=1)\n","df = df.drop('hokakisei3', axis=1)\n","df = df.drop('hokakisei4', axis=1)\n","\n","df_hokakisei = df_hokakisei.fillna(value=0)\n","\n","# 評価用データのその他規制(hokakisei1, hokakisei2, hokakisei3, hokakisei4)\n","dftest_hokakisei = pd.DataFrame()\n","\n","dftest_hokakisei = pd.concat(objs=[dftest_hokakisei, dftest['hokakisei1']], axis=1)\n","dftest_hokakisei = pd.concat(objs=[dftest_hokakisei, dftest['hokakisei2']], axis=1)\n","dftest_hokakisei = pd.concat(objs=[dftest_hokakisei, dftest['hokakisei3']], axis=1)\n","dftest_hokakisei = pd.concat(objs=[dftest_hokakisei, dftest['hokakisei4']], axis=1)\n","dftest = dftest.drop('hokakisei1', axis=1)\n","dftest = dftest.drop('hokakisei2', axis=1)\n","dftest = dftest.drop('hokakisei3', axis=1)\n","dftest = dftest.drop('hokakisei4', axis=1)\n","\n","dftest_hokakisei = dftest_hokakisei.fillna(value=0)\n","\n","# その他規制の要素種類\n","df_all_hokakisei = pd.DataFrame()\n","\n","df_all_hokakisei = pd.concat(objs=[df_hokakisei, dftest_hokakisei], axis=1)\n","\n","hokakisei_list = []\n","\n","for hokakisei_name in set(df_all_hokakisei.values.flatten()):\n","  if isinstance(hokakisei_name, int) != True and isinstance(hokakisei_name, float) != True:\n","    hokakisei_list.append(hokakisei_name)\n","\n","# 確認用  \n","#for hokakisei_name in hokakisei_list:\n","#print(hokakisei_name)\n","\n","# 学習用その他規制データ(df_hokakisei)の前処理\n","df_hokakisei_result = pd.DataFrame()\n","\n","for elem in hokakisei_list:\n","  df_hokakisei_result[elem] = df_hokakisei.apply(lambda xs: 1 if elem in xs.values else 0, axis=1)\n","\n","# 評価用その他規制データ(dftest_hokakisei)の前処理\n","dftest_hokakisei_result = pd.DataFrame()\n","\n","for elem in hokakisei_list:\n","  dftest_hokakisei_result[elem] = dftest_hokakisei.apply(lambda xs: 1 if elem in xs.values else 0, axis=1)\n","\n","df = pd.concat(objs=[df, df_hokakisei_result], axis=1)\n","dftest = pd.concat(objs=[dftest, dftest_hokakisei_result], axis=1)\n","\n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6461, 209)\n","(4273, 209)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GQxteD79CedE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1562469253365,"user_tz":-540,"elapsed":744,"user":{"displayName":"0 0","photoUrl":"","userId":"10300123485870236269"}},"outputId":"c3cd9397-0550-4b53-da9f-74efff7cf75c"},"source":["df = df.fillna(value='anyone')\n","dftest = dftest.fillna(value='anyone')\n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6461, 209)\n","(4273, 209)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zK0lKqXYZBJN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"status":"error","timestamp":1562469302586,"user_tz":-540,"elapsed":1335,"user":{"displayName":"0 0","photoUrl":"","userId":"10300123485870236269"}},"outputId":"8fd6e26e-042e-4d59-aa20-b76bafec1de3"},"source":["# onehot encoding\n","onehot = []\n","for name, type in zip(df.columns, df.dtypes):\n","  if type == 'object':\n","    onehot.append(name)\n","    \n","df = pd.get_dummies(df, prefix=onehot)\n","dftest = pd.get_dummies(dftest, prefix=onehot)\n","\n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-5234f2d09ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdftest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    823\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix_sep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mcheck_len\u001b[0;34m(item, name)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     len_msg = len_msg.format(name=name, len_item=len(item),\n\u001b[1;32m    822\u001b[0m                                              len_enc=data_to_encode.shape[1])\n\u001b[0;32m--> 823\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Length of 'prefix' (123) did not match the length of the columns being encoded (122)."]}]},{"cell_type":"code","metadata":{"id":"oWx9CJgLSb7R","colab_type":"code","colab":{}},"source":["# ペアワイズ法で欠損値がある列を削除\n","#df_dropped = pd.DataFrame()\n","#dftest_dropped = pd.DataFrame()\n","\n","#for num, name in zip(df.isnull().sum(), df.columns):\n","#  if num != 0:\n","#    df_dropped = pd.concat(objs=[df_dropped, df[name]], axis=1)\n","#    dftest_dropped = pd.concat(objs=[dftest_dropped, dftest[name]], axis=1)\n","#    df = df.drop(name, axis=1)\n","#    dftest = dftest.drop(name, axis=1)\n","    \n","#for num, name in zip(dftest.isnull().sum(), dftest.columns):\n","#  if num != 0:\n","#    df_dropped = pd.concat(objs=[df_dropped, df[name]], axis=1)\n","#    dftest_dropped = pd.concat(objs=[dftest_dropped, dftest[name]], axis=1)\n","#    df = df.drop(name, axis=1)\n","#    dftest = dftest.drop(name, axis=1)\n","    \n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA4AGCm5cTxJ","colab_type":"code","colab":{}},"source":["df = df.drop('anyone', axis=1)\n","dftest = dftest.drop('anyone', axis=1)\n","\n","# dfとdftestで異なるカラムを削除\n","common_col = (df.columns & dftest.columns)\n","# df を共通項でフィルタリング\n","df = df[common_col]\n","# dftest を共通項でフィルタリング\n","dftest = dftest[common_col]\n","    \n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU6VrEgKVFvo","colab_type":"code","colab":{}},"source":["# 文字から数字に変換する方法\n","from sklearn.preprocessing import LabelEncoder\n","\n","cat_features = []\n","\n","for i, j in zip(df.dtypes, df.columns):\n","  if i == 'object':\n","    cat_features.append(j)\n","\n","for col in cat_features:\n","  lbl = LabelEncoder()\n","  df[col] = lbl.fit_transform(list(df[col].values))\n","  dftest[col] = lbl.fit_transform(list(dftest[col].values))\n","  \n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97-qSchuVHdB","colab_type":"code","colab":{}},"source":["# 標準化\n","from sklearn.preprocessing import StandardScaler\n","\n","num_features = df.columns\n","\n","for col in num_features:\n","  scaler = StandardScaler()\n","  df[col] = scaler.fit_transform(np.array(df[col].values).reshape(-1, 1))\n","  dftest[col] = scaler.fit_transform(np.array(dftest[col].values).reshape(-1, 1))\n","  \n","print(df.shape)\n","print(dftest.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMOdDd9cfmyG","colab_type":"code","colab":{}},"source":["X_2 = df.as_matrix()\n","y = df_keiyaku_pr.as_matrix()\n","XX_2 = dftest.as_matrix()\n","\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.ensemble import RandomForestClassifier\n","est = RandomForestClassifier()\n","fs  = SelectFromModel(est)\n","fs.fit(X_2, y)\n","X_4  = fs.transform(X_2)\n","XX_4  = fs.transform(XX_2)\n","\n","print(X_4.shape)\n","print(XX_4.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMPu7yx0WHJq","colab_type":"code","colab":{}},"source":["import xgboost as xgb\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn import preprocessing, linear_model, svm\n","from sklearn.model_selection import GridSearchCV\n","\n","# モデルの作成\n","#clf = RandomForestClassifier()\n","#clf = xgb.XGBRegressor()\n","\n","xgb1 = xgb.XGBRegressor()\n","parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n","              'objective':['reg:linear'],\n","              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n","              'max_depth': [5, 6, 7],\n","              'min_child_weight': [4],\n","              'silent': [1],\n","              'subsample': [0.7],\n","              'colsample_bytree': [0.7],\n","              'n_estimators': [500]}\n","\n","clf = GridSearchCV(xgb1,\n","                   parameters,\n","                   cv = 2,\n","                   n_jobs = 5,\n","                   verbose=True)\n","\n","# 説明変数の設定\n","#X = df.as_matrix()\n","\n","# 目的変数の設定\n","Y = df_keiyaku_pr.as_matrix()\n","\n","# 学習\n","clf.fit(X_4, Y)\n","\n","print(df.shape)\n","print(dftest.shape)\n","print(dftest_id.shape)\n","print(df_keiyaku_pr.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sVwrpEeWHMH","colab_type":"code","colab":{}},"source":["#予測\n","XX = dftest.as_matrix()\n","\n","YY = clf.predict(XX_4)\n","\n","# 予測データ(df_pre)\n","df_pre = pd.DataFrame(YY)\n","df_pre.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-fdDJEOWNG7","colab_type":"code","colab":{}},"source":["# 提出用に変換\n","df_pre = df_pre.astype(int)\n","df_pre = df_pre.round()\n","df_pre = pd.concat(objs=[dftest_id, df_pre], axis=1)\n","df_pre.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NXCwqyuWPpy","colab_type":"code","colab":{}},"source":["# tsvファイル作成\n","df_pre.to_csv('test_pre_1.tsv', sep='\\t', index=False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQSGY349WQoB","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoEeA_Pmb1Lp","colab_type":"code","colab":{}},"source":["# 欠損値を埋める\n","preds_names = df_dropped.columns\n","preds_types = df_dropped.dtypes\n","\n","for preds_name, preds_type in zip(preds_names, preds_types):\n","  df_train = pd.DataFrame(pd.concat(objs=[df, df_dropped], axis=1))\n","  dftest_train = pd.DataFrame(pd.concat(objs=[df, df_dropped], axis=1))\n","  df_test_num = []\n","  dftest_test_num = []\n","  \n","  if preds_type == 'object':# 分類予測\n","    print('cla')\n","    for line_num, judg in enumerate(df_dropped[preds_name].isnull()):\n","        if judg == True: # 欠損値あり\n","            df_train = df_train.drop(line_num, axis=0)\n","            df_test_num.append(line_num)\n","\n","    for line_num, judg in enumerate(dftest_dropped[preds_name].isnull()):\n","        if judg == True: # 欠損値あり\n","            dftest_train = dftest_train.drop(line_num, axis=0)\n","            dftest_test_num.append(line_num)\n","    \n","    from sklearn.preprocessing import LabelEncoder\n","    \n","    lbl = LabelEncoder()\n","    df_train[preds_name] = lbl.fit_transform(list(df_train[preds_name].values))\n","    dftest_train[preds_name] = lbl.fit_transform(list(dftest_train[preds_name].values))\n","    df_dropped[preds_name] = lbl.fit_transform(list(df_dropped[preds_name].values))\n","    dftest_dropped[preds_name] = lbl.fit_transform(list(dftest_dropped[preds_name].values))\n","    \n","    from sklearn.preprocessing import StandardScaler\n","\n","    scaler = StandardScaler()\n","    df_train[preds_name] = scaler.fit_transform(np.array(df_train[preds_name].values).reshape(-1, 1))\n","    dftest_train[preds_name] = scaler.fit_transform(np.array(dftest_train[preds_name].values).reshape(-1, 1))\n","    df_dropped[preds_name] = scaler.fit_transform(np.array(df_dropped[preds_name].values).reshape(-1, 1))\n","    dftest_dropped[preds_name] = scaler.fit_transform(np.array(dftest_dropped[preds_name].values).reshape(-1, 1))\n","    \n","    from sklearn.ensemble import RandomForestClassifier\n","    \n","    clf = RandomForestClassifier()\n","    \n","    X = pd.DataFrame(pd.concat(objs=[df_train.iloc[:, :df_train.columns.get_loc(preds_name)], dftest_train.iloc[:, :dftest_train.columns.get_loc(preds_name)]], axis=0))\n","    T = pd.DataFrame(pd.concat(objs=[df_train[preds_name], dftest_train[preds_name]], axis=0))\n","    \n","    clf.fit(X, T)\n","    \n","    if len(df_test_num) != 0:\n","      df_dropped.loc[df_test_num, preds_name] = clf.predict(df.iloc[df_test_num, :])\n","      \n","    if len(dftest_test_num) != 0:\n","      dftest_dropped.loc[dftest_test_num, preds_name] = clf.predict(dftest.iloc[dftest_test_num, :])   \n","    \n","    df = pd.concat(objs=[df, df_dropped[preds_name]], axis=1)\n","    dftest = pd.concat(objs=[dftest, dftest_dropped[preds_name]], axis=1)\n","    df_dropped = df_dropped.drop(preds_name, axis=1)\n","    dftest_dropped = dftest_dropped.drop(preds_name, axis=1)\n","\n","    #df = pd.get_dummies(df, prefix=preds_name)\n","    #dftest = pd.get_dummies(dftest, prefix=preds_name)\n"," \n","    # dfとdftestで異なるカラムを削除\n","    common_col = (df.columns & dftest.columns)\n","    # df を共通項でフィルタリング\n","    df = df[common_col]\n","    # dftest を共通項でフィルタリング\n","    dftest = dftest[common_col]\n","        \n","  else:# 回帰予測\n","    print('reg')\n","    for line_num, judg in enumerate(df_dropped[preds_name].isnull()):\n","        if judg == True: # 欠損値あり\n","            df_train = df_train.drop(line_num, axis=0)\n","            df_test_num.append(line_num)\n","\n","    for line_num, judg in enumerate(dftest_dropped[preds_name].isnull()):\n","        if judg == True: # 欠損値あり\n","            dftest_train = dftest_train.drop(line_num, axis=0)\n","            dftest_test_num.append(line_num)\n","    \n","    from sklearn.preprocessing import LabelEncoder\n","    \n","    lbl = LabelEncoder()\n","    df_train[preds_name] = lbl.fit_transform(list(df_train[preds_name].values))\n","    dftest_train[preds_name] = lbl.fit_transform(list(dftest_train[preds_name].values))\n","    df_dropped[preds_name] = lbl.fit_transform(list(df_dropped[preds_name].values))\n","    dftest_dropped[preds_name] = lbl.fit_transform(list(dftest_dropped[preds_name].values))\n","    \n","    from sklearn.preprocessing import StandardScaler\n","\n","    scaler = StandardScaler()\n","    df_train[preds_name] = scaler.fit_transform(np.array(df_train[preds_name].values).reshape(-1, 1))\n","    dftest_train[preds_name] = scaler.fit_transform(np.array(dftest_train[preds_name].values).reshape(-1, 1))\n","    df_dropped[preds_name] = scaler.fit_transform(np.array(df_dropped[preds_name].values).reshape(-1, 1))\n","    dftest_dropped[preds_name] = scaler.fit_transform(np.array(dftest_dropped[preds_name].values).reshape(-1, 1))\n","    \n","    from sklearn.ensemble import RandomForestRegressor\n","    \n","    clf = RandomForestRegressor()\n","    \n","    X = pd.DataFrame(pd.concat(objs=[df_train.iloc[:, :df_train.columns.get_loc(preds_name)], dftest_train.iloc[:, :dftest_train.columns.get_loc(preds_name)]], axis=0))\n","    T = pd.DataFrame(pd.concat(objs=[df_train[preds_name], dftest_train[preds_name]], axis=0))\n","    \n","    clf.fit(X, T)\n","    \n","    if len(df_test_num) != 0:\n","      df_dropped.loc[df_test_num, preds_name] = clf.predict(df.iloc[df_test_num, :])\n","      \n","    if len(dftest_test_num) != 0:\n","      dftest_dropped.loc[dftest_test_num, preds_name] = clf.predict(dftest.iloc[dftest_test_num, :])\n","      \n","    df = pd.concat(objs=[df, df_dropped[preds_name]], axis=1)\n","    dftest = pd.concat(objs=[dftest, dftest_dropped[preds_name]], axis=1)\n","    df_dropped = df_dropped.drop(preds_name, axis=1)\n","    dftest_dropped = dftest_dropped.drop(preds_name, axis=1)"],"execution_count":null,"outputs":[]}]}
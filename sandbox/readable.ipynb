{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"readable.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1oPL55FxpyoX9B6fT0H_EQ4ikab3WuilA","authorship_tag":"ABX9TyMuEvu7iZyC+sZFZ7cAdJwF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JLd0AkMokMS4"},"source":["# Import library"]},{"cell_type":"code","metadata":{"id":"9NtjEyjfkPoH"},"source":["### Load data ###\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","### Create ImageDataGenerator ###\r\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UNoRMUEjpop"},"source":["# Static"]},{"cell_type":"code","metadata":{"id":"GltAbQUnjrot"},"source":["### Load data ###\r\n","PATH_TRAIN = '/content/drive/MyDrive/SANDBOX/DATASET/train_images.csv'\r\n","PATH_PREDS = '/content/drive/MyDrive/SANDBOX/DATASET/test_images.csv'\r\n","\r\n","PATH_IMAGE_TRAIN = '/content/drive/MyDrive/SANDBOX/DATASET/train_images'\r\n","PATH_IMAGE_PREDS = '/content/drive/MyDrive/SANDBOX/DATASET/test_images'\r\n","\r\n","PATH_MODEL_EFFICIENTNETB7 = '/content/drive/MyDrive/SANDBOX/DATASET/efficientnetb7.h5'\r\n","\r\n","SPLIT_TRAIN_TEST = 0.1\r\n","SPLIT_TRAIN_VALIDATION = 0.2\r\n","RANDOM_STATE = 2021\r\n","\r\n","### Create ImageDataGenerator ###\r\n","X_COL = 'id'\r\n","Y_COL = 'class_num'\r\n","IMAGE_SIZE = 640\r\n","BATCH_SIZE = 1\r\n","CLASS_MODE = 'categorical'\r\n","\r\n","### Create model ###\r\n","DROPOUT_RATE = 0.5\r\n","\r\n","### Warmup ###\r\n","EPOCHS = 100\r\n","\r\n","MONITOR_EARLYSTOPPING = 'val_loss'\r\n","MONITOR_MODELCHECKPOINT = 'val_loss'\r\n","\r\n","### Model compile & fit ###\r\n","LEARNING_RATE = 1e-5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49irkK8Rjcl9"},"source":["# Function"]},{"cell_type":"markdown","metadata":{"id":"p9xxNPWAjgUx"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"AQgzgzP-jOTg"},"source":["def load_data(PATH_TRAIN, PATH_PREDS, SPLIT_TRAIN_TEST, SPLIT_TRAIN_VALIDATION, RANDOM_STATE):\r\n","  df = pd.read_csv(PATH_TRAIN)\r\n","  df_preds = pd.read_csv(PATH_PREDS)\r\n","  \r\n","  df['id'] = PATH_IMAGE_TRAIN+'/'+df['id']\r\n","  df['class_num'] = df['class_num'].astype(str)\r\n","\r\n","  print('df shape: {0}, df_preds shape: {1}'.format(df.shape, df_preds.shape))\r\n","\r\n","  df_train, df_test = train_test_split(df, test_size=SPLIT_TRAIN_TEST, random_state=RANDOM_STATE)\r\n","  df_train, df_val = train_test_split(df_train, test_size=SPLIT_TRAIN_VALIDATION, random_state=RANDOM_STATE)\r\n","\r\n","  print('df_train shape: {0}, df_val shape: {1}, df_test shape: {2}'.format(df_train.shape, df_val.shape, df_test.shape))\r\n","\r\n","  return df_train, df_val, df_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTX84pEqmBls"},"source":["## Create ImageDataGenerator"]},{"cell_type":"code","metadata":{"id":"IngaH9ojmBrZ"},"source":["def create_imagedatagenerator(df_train, df_val, df_test, X_COL, Y_COL, IMAGE_SIZE, BATCH_SIZE, CLASS_MODE):\r\n","  train_datagen = ImageDataGenerator(featurewise_center=False,\r\n","                                    samplewise_center=False,\r\n","                                    featurewise_std_normalization=False,\r\n","                                    samplewise_std_normalization=False,\r\n","                                    zca_whitening=False,\r\n","                                    rotation_range=360,\r\n","                                    width_shift_range=1.0,\r\n","                                    height_shift_range=1.0,\r\n","                                    shear_range=0.2,\r\n","                                    zoom_range=0.2,\r\n","                                    channel_shift_range=0.0,\r\n","                                    fill_mode='nearest',\r\n","                                    cval=0.0,\r\n","                                    horizontal_flip=True,\r\n","                                    vertical_flip=True,\r\n","                                    rescale=1./255.,\r\n","                                    preprocessing_function=None,\r\n","                                    data_format=None,)\r\n","  # train_datagen = ImageDataGenerator(rescale=1./255.,)\r\n","  val_datagen = ImageDataGenerator(rescale=1./255.,)\r\n","  test_datagen = ImageDataGenerator(rescale=1./255.,)\r\n","\r\n","  train_generator = train_datagen.flow_from_dataframe(df_train, \r\n","                                                      directory=PATH_IMAGE_TRAIN, \r\n","                                                      x_col=X_COL, \r\n","                                                      y_col=Y_COL, \r\n","                                                      target_size=(IMAGE_SIZE, IMAGE_SIZE), \r\n","                                                      batch_size=BATCH_SIZE, \r\n","                                                      class_mode=CLASS_MODE)\r\n","  val_generator = val_datagen.flow_from_dataframe(df_val, \r\n","                                                  directory=PATH_IMAGE_TRAIN, \r\n","                                                  x_col=X_COL, \r\n","                                                  y_col=Y_COL, \r\n","                                                  target_size=(IMAGE_SIZE, IMAGE_SIZE), \r\n","                                                  batch_size=BATCH_SIZE, \r\n","                                                  class_mode=CLASS_MODE, \r\n","                                                  shuffle=False)\r\n","  test_generator = test_datagen.flow_from_dataframe(df_test, \r\n","                                                    directory=PATH_IMAGE_TRAIN, \r\n","                                                    x_col=X_COL, \r\n","                                                    y_col=Y_COL, \r\n","                                                    target_size=(IMAGE_SIZE, IMAGE_SIZE), \r\n","                                                    batch_size=BATCH_SIZE, \r\n","                                                    class_mode=CLASS_MODE, \r\n","                                                    shuffle=False)\r\n","\r\n","  # for train_data_batch, train_labels_batch, val_data_batch, val_labels_batch, test_data_batch, test_labels_batch in zip(train_generator, val_generator, test_generator):\r\n","  #   print('train data batch shape: {0}, val data batch shape: {1}, test data batch shape: {2}'.format(train_data_batch.shape, val_data_batch.shape, test_data_batch.shape))\r\n","  #   print('train labels batch shape: {0}, val labels batch shape: {1}, test labels batch shape: {2}'.format(train_labels_batch.shape, val_labels_batch.shape, test_labels_batch.shape))\r\n","  #   break\r\n","\r\n","  return train_generator, val_generator, test_generator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bl7xGirLoh2v"},"source":["## Create model"]},{"cell_type":"code","metadata":{"id":"rGyaHny1oh8O"},"source":["from tensorflow.keras import Input\r\n","from tensorflow.keras.applications import EfficientNetB7\r\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Activation\r\n","from keras.models import Model\r\n","\r\n","def create_model(IMAGE_SIZE, DROPOUT_RATE):\r\n","  input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='efficientnetb7')\r\n","  conv_base = EfficientNetB7(weights='imagenet', input_shape=input_layer, include_top=False)\r\n","\r\n","  ### Transfer learning ###\r\n","  for layer in conv_base.layer:\r\n","    layer.trainable = False\r\n","  \r\n","  _ = GlobalAveragePooling2D()(_)\r\n","  _ = Dropout(DROPOUT_RATE)(_)\r\n","  _ = Dense(2048)(_)\r\n","  _ = Activation('relu')(_)\r\n","  _ = Dropout(DROPOUT_RATE)(_)\r\n","  _ = Dense(4)(_)\r\n","  output_layer = Activation('softmax')(_)\r\n","\r\n","  model = Model(input_layer, output_layer)\r\n","\r\n","  model.compile()\r\n","\r\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pL8XD-wAjjbR"},"source":["# Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlEra6PgjkVl","executionInfo":{"status":"ok","timestamp":1615294178988,"user_tz":-540,"elapsed":501,"user":{"displayName":"Satoru Nakadate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOUzTMS6Ybcs-p3RWSQ9sAPSGuYFVuT8ivT9ck=s64","userId":"10300123485870236269"}},"outputId":"65304d02-c1e9-4a84-d883-1d14a4c89f02"},"source":["### Load data ###\r\n","df_train, df_val, df_test = load_data(PATH_TRAIN, PATH_PREDS, SPLIT_TRAIN_TEST, SPLIT_TRAIN_VALIDATION, RANDOM_STATE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["df shape: (1102, 2), df_preds shape: (1651, 1)\n","df_train shape: (792, 2), df_val shape: (199, 2), df_test shape: (111, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOb_U9ALoVCu","executionInfo":{"status":"ok","timestamp":1615294179911,"user_tz":-540,"elapsed":792,"user":{"displayName":"Satoru Nakadate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOUzTMS6Ybcs-p3RWSQ9sAPSGuYFVuT8ivT9ck=s64","userId":"10300123485870236269"}},"outputId":"a60b2470-1e3c-4586-dc31-5aa27879e603"},"source":["### Create ImageDataGenerator ###\r\n","train_generator, val_generator, test_generator = create_imagedatagenerator(df_train, df_val, df_test, X_COL, Y_COL, IMAGE_SIZE, BATCH_SIZE, CLASS_MODE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 792 validated image filenames belonging to 4 classes.\n","Found 199 validated image filenames belonging to 4 classes.\n","Found 111 validated image filenames belonging to 4 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTZsZHR0od6H","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"error","timestamp":1615294182386,"user_tz":-540,"elapsed":516,"user":{"displayName":"Satoru Nakadate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOUzTMS6Ybcs-p3RWSQ9sAPSGuYFVuT8ivT9ck=s64","userId":"10300123485870236269"}},"outputId":"7192431c-481b-412b-f7f4-bb7a6e79beb6"},"source":["### Create model ###\r\n","model = create_model(IMAGE_SIZE, DROPOUT_RATE)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-04398a73875b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Create model ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-14dba7a9bff2>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(IMAGE_SIZE, DROPOUT_RATE)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'efficientnetb7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mconv_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m### Transfer learning ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNetB7\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m       \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       weights=weights)\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    345\u001b[0m                          '`input_shape` should be ' + str(default_shape) + '.')\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     raise TypeError('Keras symbolic inputs/outputs do not '\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0;34m'implement `__len__`. You may be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0;34m'trying to pass Keras symbolic inputs/outputs '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."]}]},{"cell_type":"markdown","metadata":{"id":"4yOC01TjtuJ3"},"source":["## Warmup"]},{"cell_type":"code","metadata":{"id":"PhslccXstUUn"},"source":["!pip install tensorflow_addons\r\n","from tensorflow_addons.optimizers import RectifiedAdam\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n","import time\r\n","\r\n","callbacks_list = [EarlyStopping(monitor=MONITOR_EARLYSTOPPING, patience=5, mode='min',),\r\n","                  CustomCallback(model, val_generator),\r\n","                  ModelCheckpoint(filepath=MODEL_PATH, monitor=MONITOR_MODELCHECKPOINT, save_best_only=True, mode='min',),]\r\n","\r\n","model.compile(optimizer=RectifiedAdam(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5),\r\n","              loss='categorical_crossentropy',\r\n","              metrics=['accuracy',],)\r\n","\r\n","model.fit(train_generator,\r\n","          epochs=EPOCHS_WARMUP,\r\n","          batch_size=BATCH_SIZE,\r\n","          callbacks=callbacks_list,\r\n","          validation_data=val_generator,)\r\n","\r\n","from keras.models import load_model\r\n","\r\n","model = load_model(MODEL_PATH_EFFICIENTNETB7)\r\n","\r\n","for layer in model.layers:\r\n","    layer.trainable = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rtWOQdjItzUb"},"source":["## TRAIN"]},{"cell_type":"code","metadata":{"id":"IZYCD-MDtzgC"},"source":["callbacks_list = [EarlyStopping(monitor=MONITOR_EARLYSTOPPING, patience=5, mode='min',),\r\n","                  CustomCallback(model, val_generator),\r\n","                  ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-6, verbose=1)\r\n","                  ModelCheckpoint(filepath=MODEL_PATH, monitor=MONITOR_MODELCHECKPOINT, save_best_only=True, mode='min',),]\r\n","\r\n","model.compile(optimizer=Adam(lr=LEARNING_RATE), loss='categorical_crossentropy',  metrics=['accuracy'])\r\n","\r\n","model.fit(train_generator,\r\n","          epochs=EPOCHS_WARMUP,\r\n","          batch_size=BATCH_SIZE,\r\n","          callbacks=callbacks_list,\r\n","          validation_data=val_generator,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMFz9o93w5i3"},"source":["### Evaluation ###\r\n","import matplotlib.pyplot as plt\r\n","\r\n","acc = history.history['accuracy']\r\n","val_acc = history.history['val_accuracy']\r\n","loss = history.history['loss']\r\n","val_loss = history.history['val_loss']\r\n","epochs = range(1, len(acc) + 1)\r\n","\r\n","fig = plt.figure(0)\r\n","\r\n","plt.plot(epochs, acc, 'b', label='Training acc')\r\n","plt.plot(epochs, val_acc, 'b', color='orange', label='Validation acc')\r\n","plt.title('Training and validation accuracy')\r\n","plt.legend()\r\n","\r\n","plt.show()\r\n","\r\n","fig = plt.figure(1)\r\n","\r\n","plt.plot(epochs, loss, 'b', label='Training loss')\r\n","plt.plot(epochs, val_loss, 'b', color='orange', label='Validation loss')\r\n","plt.title('Training and validation loss')\r\n","plt.legend()\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCiOTyVkxW9P"},"source":["from tensorflow_addons.metrics import CohenKappa\r\n","from keras.models import load_model\r\n","\r\n","clf = load_model(MODEL_PATH_EFFICIENTNETB7)\r\n","\r\n","import numpy as np\r\n","from sklearn.metrics import confusion_matrix \r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","\r\n","predicted = np.argmax(clf.predict(test_generator), axis=1)\r\n","test_labels = test_generator.classes\r\n","\r\n","cf_matrix = confusion_matrix(test_labels, predicted)\r\n","\r\n","fig = plt.figure(figsize=(16, 8))\r\n","\r\n","sns.heatmap(cf_matrix, annot=True, cmap='Greens', xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES)\r\n","\r\n","plt.xlabel('Predicted')\r\n","plt.ylabel('Actual')\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcWc-vuoxj5k"},"source":["from sklearn.metrics import cohen_kappa_score, accuracy_score\r\n","\r\n","print('Test cohen kappa score: %.3f' % cohen_kappa_score(np.argmax(clf.predict(test_generator), axis=1), test_generator.classes, weights='quadratic'))\r\n","print('Test accuracy score : %.3f' % accuracy_score(np.argmax(clf.predict(test_generator), axis=1), test_generator.classes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEJL3_9yxpgr"},"source":["### Submission ###\r\n","import numpy as np\r\n","from PIL import Image\r\n","\r\n","preds = []\r\n","\r\n","for image_id in df_preds['id']:\r\n","    image = Image.open(TEST_IMAGE_PATH+'/'+image_id)\r\n","    image = np.expand_dims(image, axis=0)\r\n","    image = image / 255.\r\n","    preds.append(np.argmax(clf.predict(image)))\r\n","\r\n","df_preds['label'] = preds\r\n","\r\n","df_preds.to_csv('/content/drive/MyDrive/SANDBOX/DATASET/submission.csv', index=False, header=False)\r\n","\r\n","df_preds.head()"],"execution_count":null,"outputs":[]}]}